=== Starting Training 3D Lowres Folds 1-4 ===

[1/4] Training Fold 1...
Traceback (most recent call last):
  File "C:\CardiacSeg\.venv\Lib\site-packages\batchgenerators\dataloading\nondet_multi_threaded_augmenter.py", line 53, in producer
    item = next(data_loader)
           ^^^^^^^^^^^^^^^^^
  File "C:\CardiacSeg\.venv\Lib\site-packages\batchgenerators\dataloading\data_loader.py", line 126, in __next__
    return self.generate_train_batch()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\CardiacSeg\.venv\Lib\site-packages\nnunetv2\training\dataloading\data_loader.py", line 170, in generate_train_batch
    data_all = np.zeros(self.data_shape, dtype=np.float32)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
numpy._core._exceptions._ArrayMemoryError: Unable to allocate 82.0 MiB for an array with shape (2, 1, 191, 257, 219) and data type float32
Exception in background worker 9:
 Unable to allocate 82.0 MiB for an array with shape (2, 1, 191, 257, 219) and data type float32
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "C:\CardiacSeg\.venv\Scripts\nnUNetv2_train.exe\__main__.py", line 6, in <module>
  File "C:\CardiacSeg\.venv\Lib\site-packages\nnunetv2\run\run_training.py", line 266, in run_training_entry
    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,
  File "C:\CardiacSeg\.venv\Lib\site-packages\nnunetv2\run\run_training.py", line 207, in run_training
    nnunet_trainer.run_training()
  File "C:\CardiacSeg\.venv\Lib\site-packages\nnunetv2\training\nnUNetTrainer\nnUNetTrainer.py", line 1371, in run_training
    train_outputs.append(self.train_step(next(self.dataloader_train)))
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\CardiacSeg\.venv\Lib\site-packages\batchgenerators\dataloading\nondet_multi_threaded_augmenter.py", line 196, in __next__
    item = self.__get_next_item()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\CardiacSeg\.venv\Lib\site-packages\batchgenerators\dataloading\nondet_multi_threaded_augmenter.py", line 181, in __get_next_item
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
Exception in thread Thread-2 (results_loop):
Traceback (most recent call last):
  File "C:\Users\tyler\AppData\Local\Programs\Python\Python311\Lib\threading.py", line 1045, in _bootstrap_inner
    self.run()
  File "C:\Users\tyler\AppData\Local\Programs\Python\Python311\Lib\threading.py", line 982, in run
    self._target(*self._args, **self._kwargs)
  File "C:\CardiacSeg\.venv\Lib\site-packages\batchgenerators\dataloading\nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "C:\CardiacSeg\.venv\Lib\site-packages\batchgenerators\dataloading\nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message

############################
INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md
############################

Using device: cuda:0

#######################################################################
Please cite the following paper when using nnU-Net:
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.
#######################################################################

2025-11-23 22:45:52.770239: do_dummy_2d_data_aug: False
2025-11-23 22:45:52.773307: Using splits from existing split file: C:\CardiacSeg\nnUNet_preprocessed\Dataset001_CardiacSeg\splits_final.json
2025-11-23 22:45:52.790569: The split file contains 5 splits.
2025-11-23 22:45:52.801268: Desired fold for training: 1
2025-11-23 22:45:52.812930: This split has 40 training and 10 validation cases.
using pin_memory on device 0
using pin_memory on device 0

This is the configuration used by this training:
Configuration name: 3d_lowres
 {'data_identifier': 'nnUNetPlans_3d_lowres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [96, 160, 160], 'median_image_size_in_voxels': [149, 252, 252], 'spacing': [1.0163970532302022, 0.8020007998144563, 0.8020007998144563], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': False, 'next_stage': '3d_cascade_fullres'} 

These are the global plan.json settings:
 {'dataset_name': 'Dataset001_CardiacSeg', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [0.5, 0.39453125, 0.39453125], 'original_median_shape_after_transp': [340, 512, 512], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1819.0, 'mean': 136.5193634033203, 'median': 115.0, 'min': -882.0, 'percentile_00_5': -31.0, 'percentile_99_5': 478.0, 'std': 91.3119888305664}}} 

2025-11-23 22:46:21.566847: Unable to plot network architecture:
2025-11-23 22:46:21.585027: No module named 'hiddenlayer'
2025-11-23 22:46:21.653742: 
2025-11-23 22:46:21.664994: Epoch 0
2025-11-23 22:46:21.676831: Current learning rate: 0.01
2025-11-23 22:48:45.856388: train_loss 0.0205
2025-11-23 22:48:45.874142: val_loss -0.1435
2025-11-23 22:48:45.885296: Pseudo dice [np.float32(0.8116), np.float32(0.0), np.float32(0.0)]
2025-11-23 22:48:45.896980: Epoch time: 144.2 s
2025-11-23 22:48:45.907109: Yayy! New best EMA pseudo Dice: 0.2705000042915344
2025-11-23 22:48:47.005842: 
2025-11-23 22:48:47.020696: Epoch 1
2025-11-23 22:48:47.031985: Current learning rate: 0.00999
2025-11-23 22:50:59.739542: train_loss -0.1666
2025-11-23 22:50:59.747686: val_loss -0.2381
2025-11-23 22:50:59.756910: Pseudo dice [np.float32(0.8529), np.float32(0.0), np.float32(0.0)]
2025-11-23 22:50:59.764533: Epoch time: 132.73 s
2025-11-23 22:50:59.772261: Yayy! New best EMA pseudo Dice: 0.2718999981880188
2025-11-23 22:51:00.931638: 
2025-11-23 22:51:00.943076: Epoch 2
2025-11-23 22:51:00.953919: Current learning rate: 0.00998
2025-11-23 22:53:19.969060: train_loss -0.291
2025-11-23 22:53:20.062182: val_loss -0.3479
2025-11-23 22:53:20.073833: Pseudo dice [np.float32(0.8627), np.float32(0.576), np.float32(0.0)]
2025-11-23 22:53:20.085504: Epoch time: 139.04 s
2025-11-23 22:53:20.098213: Yayy! New best EMA pseudo Dice: 0.29269999265670776
2025-11-23 22:53:23.391935: 
2025-11-23 22:53:23.406135: Epoch 3
2025-11-23 22:53:23.417809: Current learning rate: 0.00997
2025-11-23 22:55:42.918451: train_loss -0.3648
2025-11-23 22:55:42.931113: val_loss -0.3735
2025-11-23 22:55:42.943270: Pseudo dice [np.float32(0.8656), np.float32(0.6326), np.float32(0.0)]
2025-11-23 22:55:42.955482: Epoch time: 139.53 s
2025-11-23 22:55:42.966645: Yayy! New best EMA pseudo Dice: 0.313400000333786
2025-11-23 22:55:44.547966: 
2025-11-23 22:55:44.562678: Epoch 4
2025-11-23 22:55:44.573824: Current learning rate: 0.00996
2025-11-23 22:58:06.853166: train_loss -0.3839
2025-11-23 22:58:06.854170: val_loss -0.3758
2025-11-23 22:58:06.865707: Pseudo dice [np.float32(0.8747), np.float32(0.6396), np.float32(0.0)]
2025-11-23 22:58:06.876740: Epoch time: 142.31 s
2025-11-23 22:58:06.888273: Yayy! New best EMA pseudo Dice: 0.33250001072883606
2025-11-23 22:58:08.224846: 
2025-11-23 22:58:08.238136: Epoch 5
2025-11-23 22:58:08.249837: Current learning rate: 0.00995
2025-11-23 23:00:31.806228: train_loss -0.4034
2025-11-23 23:00:31.807233: val_loss -0.4075
2025-11-23 23:00:31.818990: Pseudo dice [np.float32(0.8818), np.float32(0.6883), np.float32(0.0)]
2025-11-23 23:00:31.830252: Epoch time: 143.58 s
2025-11-23 23:00:31.841494: Yayy! New best EMA pseudo Dice: 0.3515999913215637
2025-11-23 23:00:33.130596: 
2025-11-23 23:00:33.145960: Epoch 6
2025-11-23 23:00:33.158702: Current learning rate: 0.00995
2025-11-23 23:02:55.691383: train_loss -0.4102
2025-11-23 23:02:55.692893: val_loss -0.4115
2025-11-23 23:02:55.704063: Pseudo dice [np.float32(0.889), np.float32(0.6997), np.float32(0.0)]
2025-11-23 23:02:55.716218: Epoch time: 142.56 s
2025-11-23 23:02:55.726848: Yayy! New best EMA pseudo Dice: 0.3693999946117401
2025-11-23 23:02:56.930475: 
2025-11-23 23:02:56.947271: Epoch 7
2025-11-23 23:02:56.959557: Current learning rate: 0.00994
2025-11-23 23:05:11.099260: train_loss -0.4253
2025-11-23 23:05:11.100798: val_loss -0.4238
2025-11-23 23:05:11.113021: Pseudo dice [np.float32(0.891), np.float32(0.7056), np.float32(0.0)]
2025-11-23 23:05:11.123884: Epoch time: 134.17 s
2025-11-23 23:05:11.134138: Yayy! New best EMA pseudo Dice: 0.385699987411499
2025-11-23 23:05:12.303875: 
2025-11-23 23:05:12.318592: Epoch 8
2025-11-23 23:05:12.331285: Current learning rate: 0.00993
2025-11-23 23:07:33.998487: train_loss -0.4325
2025-11-23 23:07:33.999490: val_loss -0.442
2025-11-23 23:07:34.016254: Pseudo dice [np.float32(0.8982), np.float32(0.7242), np.float32(0.0)]
2025-11-23 23:07:34.026909: Epoch time: 141.7 s
2025-11-23 23:07:34.037063: Yayy! New best EMA pseudo Dice: 0.40119999647140503
2025-11-23 23:07:35.310361: 
2025-11-23 23:07:35.325840: Epoch 9
2025-11-23 23:07:35.338275: Current learning rate: 0.00992
2025-11-23 23:09:58.201378: train_loss -0.4353
2025-11-23 23:09:58.202410: val_loss -0.4357
2025-11-23 23:09:58.216404: Pseudo dice [np.float32(0.895), np.float32(0.7049), np.float32(0.0)]
2025-11-23 23:09:58.227668: Epoch time: 142.89 s
2025-11-23 23:09:58.238910: Yayy! New best EMA pseudo Dice: 0.41440001130104065
2025-11-23 23:09:59.484400: 
2025-11-23 23:09:59.497709: Epoch 10
2025-11-23 23:09:59.511511: Current learning rate: 0.00991
2025-11-23 23:12:23.726123: train_loss -0.4478
2025-11-23 23:12:23.728155: val_loss -0.4339
2025-11-23 23:12:23.741369: Pseudo dice [np.float32(0.9039), np.float32(0.7218), np.float32(0.0)]
2025-11-23 23:12:23.751515: Epoch time: 144.24 s
2025-11-23 23:12:23.761711: Yayy! New best EMA pseudo Dice: 0.4271000027656555
2025-11-23 23:12:25.298822: 
2025-11-23 23:12:25.311526: Epoch 11
2025-11-23 23:12:25.322765: Current learning rate: 0.0099
2025-11-23 23:14:47.071754: train_loss -0.4452
2025-11-23 23:14:47.085065: val_loss -0.441
2025-11-23 23:14:47.095354: Pseudo dice [np.float32(0.8949), np.float32(0.7235), np.float32(0.0)]
2025-11-23 23:14:47.105526: Epoch time: 141.77 s
2025-11-23 23:14:47.115180: Yayy! New best EMA pseudo Dice: 0.438400000333786
2025-11-23 23:14:48.571313: 
2025-11-23 23:14:48.584004: Epoch 12
2025-11-23 23:14:48.593091: Current learning rate: 0.00989
2025-11-23 23:17:08.922631: train_loss -0.4495
2025-11-23 23:17:08.935169: val_loss -0.4377
2025-11-23 23:17:08.947716: Pseudo dice [np.float32(0.899), np.float32(0.7284), np.float32(0.0)]
2025-11-23 23:17:08.958256: Epoch time: 140.35 s
2025-11-23 23:17:08.970811: Yayy! New best EMA pseudo Dice: 0.4487999975681305
2025-11-23 23:17:10.259051: 
2025-11-23 23:17:10.275102: Epoch 13
2025-11-23 23:17:10.286647: Current learning rate: 0.00988
2025-11-23 23:19:34.434740: train_loss -0.4575
2025-11-23 23:19:34.434740: val_loss -0.4564
2025-11-23 23:19:34.446956: Pseudo dice [np.float32(0.9013), np.float32(0.7322), np.float32(0.0)]
2025-11-23 23:19:34.457688: Epoch time: 144.18 s
2025-11-23 23:19:34.467970: Yayy! New best EMA pseudo Dice: 0.45840001106262207
2025-11-23 23:19:35.734907: 
2025-11-23 23:19:35.746620: Epoch 14
2025-11-23 23:19:35.758367: Current learning rate: 0.00987
2025-11-23 23:21:46.878026: train_loss -0.4577
2025-11-23 23:21:46.878538: val_loss -0.4579
2025-11-23 23:21:46.891234: Pseudo dice [np.float32(0.9026), np.float32(0.7392), np.float32(0.0)]
2025-11-23 23:21:46.901361: Epoch time: 131.14 s
2025-11-23 23:21:46.910986: Yayy! New best EMA pseudo Dice: 0.46720001101493835
2025-11-23 23:21:48.181825: 
2025-11-23 23:21:48.192508: Epoch 15
2025-11-23 23:21:48.202666: Current learning rate: 0.00986
2025-11-23 23:24:02.873083: train_loss -0.4604
2025-11-23 23:24:02.874613: val_loss -0.4484
2025-11-23 23:24:02.889335: Pseudo dice [np.float32(0.902), np.float32(0.7269), np.float32(0.0)]
2025-11-23 23:24:02.901496: Epoch time: 134.69 s
2025-11-23 23:24:02.914895: Yayy! New best EMA pseudo Dice: 0.4747999906539917
2025-11-23 23:24:04.185039: 
2025-11-23 23:24:04.198823: Epoch 16
2025-11-23 23:24:04.211131: Current learning rate: 0.00986
2025-11-23 23:26:27.580616: train_loss -0.4556
2025-11-23 23:26:27.582126: val_loss -0.4628
2025-11-23 23:26:27.596049: Pseudo dice [np.float32(0.9038), np.float32(0.7376), np.float32(0.0)]
2025-11-23 23:26:27.609102: Epoch time: 143.4 s
2025-11-23 23:26:27.620881: Yayy! New best EMA pseudo Dice: 0.4821000099182129
2025-11-23 23:26:28.876228: 
2025-11-23 23:26:28.889438: Epoch 17
2025-11-23 23:26:28.903119: Current learning rate: 0.00985
2025-11-23 23:28:51.437287: train_loss -0.4676
2025-11-23 23:28:51.456334: val_loss -0.4601
2025-11-23 23:28:51.471378: Pseudo dice [np.float32(0.9075), np.float32(0.7431), np.float32(0.0)]
2025-11-23 23:28:51.484917: Epoch time: 142.56 s
2025-11-23 23:28:51.500959: Yayy! New best EMA pseudo Dice: 0.48890000581741333
2025-11-23 23:28:53.005808: 
2025-11-23 23:28:53.025748: Epoch 18
2025-11-23 23:28:53.043599: Current learning rate: 0.00984
2025-11-23 23:31:13.643732: train_loss -0.4728
2025-11-23 23:31:13.645240: val_loss -0.4499
2025-11-23 23:31:13.662494: Pseudo dice [np.float32(0.9052), np.float32(0.7213), np.float32(0.0)]
2025-11-23 23:31:13.674177: Epoch time: 140.64 s
2025-11-23 23:31:13.690448: Yayy! New best EMA pseudo Dice: 0.4941999912261963
2025-11-23 23:31:14.953027: 
2025-11-23 23:31:14.967820: Epoch 19
2025-11-23 23:31:14.981167: Current learning rate: 0.00983
2025-11-23 23:33:37.286000: train_loss -0.4664
2025-11-23 23:33:37.286518: val_loss -0.4614
2025-11-23 23:33:37.299728: Pseudo dice [np.float32(0.9028), np.float32(0.7462), np.float32(0.0)]
2025-11-23 23:33:37.309884: Epoch time: 142.33 s
2025-11-23 23:33:37.321084: Yayy! New best EMA pseudo Dice: 0.49970000982284546
2025-11-23 23:33:38.627567: 
2025-11-23 23:33:38.648830: Epoch 20
2025-11-23 23:33:38.668180: Current learning rate: 0.00982
Exception ignored in: <function NonDetMultiThreadedAugmenter.__del__ at 0x000002701C2D6520>
Traceback (most recent call last):
  File "C:\CardiacSeg\.venv\Lib\site-packages\batchgenerators\dataloading\nondet_multi_threaded_augmenter.py", line 259, in __del__
    self._finish()
  File "C:\CardiacSeg\.venv\Lib\site-packages\batchgenerators\dataloading\nondet_multi_threaded_augmenter.py", line 244, in _finish
    self.abort_event.set()
  File "C:\Users\tyler\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\synchronize.py", line 345, in set
    self._cond.notify_all()
  File "C:\Users\tyler\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\synchronize.py", line 304, in notify_all
    self.notify(n=sys.maxsize)
  File "C:\Users\tyler\AppData\Local\Programs\Python\Python311\Lib\multiprocessing\synchronize.py", line 279, in notify
    assert not self._wait_semaphore.acquire(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [WinError 6] The handle is invalid
train_remaining_3d_lowres.ps1: Training Fold 1 failed
