# cite: Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring 
# method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.

import os
import subprocess
import sys
from pathlib import Path
import multiprocessing
import json
import argparse
try:
    from tqdm import tqdm
except ImportError:
    print("å®‰è£ tqdm ä»¥é¡¯ç¤ºé€²åº¦æ¢...")
    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tqdm'])
    from tqdm import tqdm

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# è·¨å¹³å°ï¼šä½¿ç”¨ç•¶å‰è…³æœ¬æ‰€åœ¨ç›®éŒ„ä½œç‚º base_dir
base_dir = os.path.dirname(os.path.abspath(__file__))
print(f"è…³æœ¬ base_dir: {base_dir}")  # è¨ºæ–·æ‰“å°

# nnU-Net v2 ç’°å¢ƒè®Šæ•¸è¨­å®šï¼ˆåŒæ™‚è¨­å®šå¤§å°å¯«ï¼Œé¿å…ä¸åŒç‰ˆæœ¬å¼•ç”¨ä¸åŒåç¨±ï¼‰
_raw_dir = os.path.join(base_dir, "nnUNet_raw")
_prep_dir = os.path.join(base_dir, "nnUNet_preprocessed")
_res_dir = os.path.join(base_dir, "nnUNet_results")
os.environ["NNUNET_RAW"] = _raw_dir
os.environ["NNUNET_PREPROCESSED"] = _prep_dir
os.environ["NNUNET_RESULTS"] = _res_dir
os.environ["nnUNet_raw"] = _raw_dir
os.environ["nnUNet_preprocessed"] = _prep_dir
os.environ["nnUNet_results"] = _res_dir

# è‹¥åµæ¸¬åˆ°ä½¿ç”¨ç³»çµ± Python è€Œéè™›æ“¬ç’°å¢ƒï¼Œçµ¦å‡ºæé†’
if ".venv" not in sys.executable:
    print("WARNING: ç›®å‰ä½¿ç”¨çš„ Python ä¸¦éå°ˆæ¡ˆè™›æ“¬ç’°å¢ƒ (.venv)ã€‚å»ºè­°å…ˆåŸ·è¡Œ: source .venv/bin/activate æˆ–ä½¿ç”¨ ./.venv/bin/pythonã€‚")

# æ•¸æ“šé›† ID å’Œåç¨±
dataset_id = "001"
dataset_name = "CardiacSeg"
dataset_dir = os.path.join(os.environ["NNUNET_RAW"], f"Dataset{dataset_id}_{dataset_name}")

def install_dependencies():
    required = ["nnunetv2", "blosc2"]
    for pkg in required:
        try:
            __import__(pkg)
        except ImportError:
            print(f"å®‰è£ç¼ºå¤±å¥—ä»¶: {pkg} ...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])

def generate_dataset_json(force_regenerate=False):
    json_path = os.path.join(dataset_dir, "dataset.json")
    if os.path.exists(json_path) and not force_regenerate:
        print(f"dataset.json å·²å­˜åœ¨æ–¼ {json_path}ï¼Œè·³éç”Ÿæˆã€‚è‹¥éœ€å¼·åˆ¶é‡æ–°ç”Ÿæˆï¼Œè«‹è¨­å®š force_regenerate=Trueã€‚")
        return

    # å‹•æ…‹æƒæ imagesTr å’Œ labelsTr ä»¥ç”Ÿæˆ training æ¸…å–®ï¼ˆåŒ¹é…ç”¨æˆ¶æª”åæ¨¡å¼ï¼špatientXXXX.nii.gz å’Œ patientXXXX_gt.nii.gzï¼‰
    images_tr_dir = os.path.join(dataset_dir, "imagesTr")
    labels_tr_dir = os.path.join(dataset_dir, "labelsTr")
    images_ts_dir = os.path.join(dataset_dir, "imagesTs")

    training = []
    for img_file in sorted(os.listdir(images_tr_dir)):
        if img_file.endswith(".nii.gz"):
            # å‡è¨­å½±åƒæª”åå¦‚ patient0001.nii.gzï¼Œè½‰æ›ç‚º nnU-Net æ¨™æº–æ ¼å¼ï¼ˆæ·»åŠ  _0000 è‹¥ç¼ºå°‘ï¼‰
            if '_' not in img_file or not img_file.split('_')[-1].startswith('0000'):
                # è­¦å‘Šï¼šnnU-Net æœŸæœ›å½±åƒæª”åå¦‚ patient0001_0000.nii.gzï¼ˆå–®æ¨¡æ…‹ï¼‰
                print(f"è­¦å‘Šï¼šå½±åƒæª” {img_file} ç¼ºå°‘ _0000 å¾Œç¶´ã€‚å»ºè­°é‡å‘½åç‚º {img_file.replace('.nii.gz', '_0000.nii.gz')} ä»¥ç¬¦åˆ nnU-Net æ¨™æº–ã€‚")
                std_img_file = img_file.replace('.nii.gz', '_0000.nii.gz')
            else:
                std_img_file = img_file

            # æ¨™ç±¤æª”åï¼špatientXXXX_gt.nii.gzï¼ŒnnU-Net æœŸæœ›ç„¡ _gtï¼ˆå¦‚ patient0001.nii.gzï¼‰
            case_id = img_file.split('.')[0]  # e.g., patient0001
            label_file = f"{case_id}_gt.nii.gz"
            std_label_file = f"{case_id}.nii.gz"  # å»ºè­°ç§»é™¤ _gt
            if os.path.exists(os.path.join(labels_tr_dir, label_file)):
                print(f"è­¦å‘Šï¼šæ¨™ç±¤æª” {label_file} æœ‰ _gt å¾Œç¶´ã€‚å»ºè­°é‡å‘½åç‚º {std_label_file} ä»¥ç¬¦åˆ nnU-Net æ¨™æº–ã€‚")
                training.append({"image": f"./imagesTr/{std_img_file}", "label": f"./labelsTr/{std_label_file}"})

    test = [f"./imagesTs/{f.replace('.nii.gz', '_0000.nii.gz') if '_' not in f or not f.split('_')[-1].startswith('0000') else f}" 
            for f in sorted(os.listdir(images_ts_dir)) if f.endswith(".nii.gz")]

    dataset_json = {
        "channel_names": {"0": "CT"},  # èª¿æ•´ç‚ºæ‚¨çš„æ¨¡æ…‹ï¼Œä¾‹å¦‚ "MRI"
        "labels": {
            "background": 0,
            "label1": 1,  # èª¿æ•´ç‚ºå¯¦éš›æ¨™ç±¤ï¼Œä¾‹å¦‚ "heart": 1, "aorta": 2 ç­‰
            # æ·»åŠ æ›´å¤šæ¨™ç±¤ï¼Œå¦‚ "myocardium": 2 ç­‰
        },
        "numTraining": len(training),
        "file_ending": ".nii.gz",
        "name": dataset_name,
        "description": "Cardiac segmentation dataset",
        "reference": "Your reference",
        "licence": "Your licence",
        "release": "1.0",
        "tensorImageSize": "3D",  # æˆ– "4D" è‹¥ç‚ºæ™‚åºè³‡æ–™
        "training": training,
        "test": test
    }

    with open(json_path, 'w') as f:
        json.dump(dataset_json, f, indent=4)
    print(f"å·²ç”Ÿæˆ/æ›´æ–° dataset.json æ–¼ {json_path}ã€‚è«‹æ‰‹å‹•é©—è­‰å…§å®¹ï¼Œä¸¦é‡å‘½åæª”æ¡ˆä»¥åŒ¹é… nnU-Net æ¨™æº–ï¼ˆå½±åƒ: case_0000.nii.gzï¼Œæ¨™ç±¤: case.nii.gzï¼‰ï¼")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='nnU-Net è¨“ç·´è…³æœ¬ï¼ˆæ”¯æ´è‡ªè¨‚åƒæ•¸ï¼‰')
    parser.add_argument('--epochs', type=int, default=250, help='è¨“ç·´çš„ epoch æ•¸é‡ï¼ˆé è¨­: 250ï¼ŒnnUNet åŸå§‹é è¨­ 1000ï¼‰')
    parser.add_argument('--fold', type=str, default='0', help='è¨“ç·´çš„ foldï¼ˆ0-4 æˆ– allï¼Œé è¨­: 0ï¼‰')
    parser.add_argument('--config', type=str, default='3d_fullres', help='è¨“ç·´é…ç½®ï¼ˆ2d, 3d_fullres, 3d_lowresï¼Œé è¨­: 3d_fullresï¼‰')
    parser.add_argument('--device', type=str, default='mps', help='ä½¿ç”¨çš„è£ç½®ï¼ˆcuda, mps, cpuï¼Œé è¨­: mpsï¼‰')
    parser.add_argument('--continue-training', action='store_true', help='å¾æœ€æ–° checkpoint ç¹¼çºŒè¨“ç·´')
    parser.add_argument('--skip-preprocess', action='store_true', help='è·³éé è™•ç†æ­¥é©Ÿï¼ˆå‡è¨­å·²å®Œæˆï¼‰')
    args = parser.parse_args()

    multiprocessing.freeze_support()
    install_dependencies()

    # ç”Ÿæˆ dataset.json è‹¥ç¼ºå°‘ï¼ˆä¿ç•™ç¾æœ‰ json ä¸è¦†è“‹ï¼‰
    generate_dataset_json(force_regenerate=False)

    # æ³¨æ„ï¼šç”¨æˆ¶å·²æ‰‹å‹•çµ„ç¹”æ•¸æ“šï¼Œå› æ­¤è·³é organize_data()

    if not args.skip_preprocess:
        print("\n" + "="*60)
        print("ğŸ”§ æ­¥é©Ÿ 1: è¦åŠƒå’Œé è™•ç†æ•¸æ“š")
        print("="*60)
        print(f"è³‡æ–™é›† ID: {dataset_id}")
        print(f"è³‡æ–™é›†åç¨±: {dataset_name}")
        print(f"åŸå§‹è³‡æ–™: {os.environ['NNUNET_RAW']}")
        print(f"é è™•ç†è³‡æ–™: {os.environ['NNUNET_PREPROCESSED']}")
        print("\né–‹å§‹åŸ·è¡Œ nnUNetv2_plan_and_preprocess...\n")
        
        # æ­¥é©Ÿ 1: è¦åŠƒå’Œé è™•ç†æ•¸æ“šï¼ˆåªéœ€é‹è¡Œä¸€æ¬¡ï¼‰
        subprocess.run([
            "nnUNetv2_plan_and_preprocess",
            "-d", dataset_id,
            "--verify_dataset_integrity"
        ], check=True)
    else:
        print("\n>> è·³éé è™•ç†æ­¥é©Ÿï¼ˆä½¿ç”¨ --skip-preprocessï¼‰")

    print("\n" + "="*60)
    print(">> æ­¥é©Ÿ 2: é–‹å§‹è¨“ç·´æ¨¡å‹")
    print("="*60)
    print(f"è³‡æ–™é›† ID: {dataset_id}")
    print(f"é…ç½®: {args.config}")
    print(f"Fold: {args.fold}")
    print(f"Epochs: {args.epochs}")
    print(f"è£ç½®: {args.device}")
    print(f"ç¹¼çºŒè¨“ç·´: {'æ˜¯' if args.continue_training else 'å¦'}")
    print(f"çµæœå„²å­˜: {os.environ['NNUNET_RESULTS']}")
    print("\né–‹å§‹åŸ·è¡Œ nnUNetv2_train...\n")
    
    # æ­¥é©Ÿ 2: è¨“ç·´
    # ä½¿ç”¨ç•¶å‰ Python ç›´è­¯å™¨ä»¥é¿å… PATH å°æ‡‰åˆ°ç³»çµ±å®‰è£çš„èˆŠç‰ˆ/éŒ¯èª¤ç’°å¢ƒ
    train_cmd = [
        sys.executable, "-m", "nnunetv2.run.run_training",
        dataset_id, args.config, args.fold,
        "-p", "nnUNetPlans",
        "-device", args.device,
        "--npz"
    ]
    
    # è¨­å®šè‡ªè¨‚ epochsï¼ˆé€éç’°å¢ƒè®Šæ•¸ï¼ŒnnUNet æœƒè®€å–ï¼‰
    train_env = os.environ.copy()
    train_env["nnUNet_n_epochs"] = str(args.epochs)
    # nnUNet (and dependencies) may set environment variables expecting string values.
    # Ensure numeric environment variables are strings to avoid TypeError: str expected, not int
    # Example: running nnunetv2.run.run_training may attempt `os.environ['TORCHINDUCTOR_COMPILE_THREADS'] = 1`
    # which raises when the right-hand side is an int. Provide a safe string value here.
    train_env["TORCHINDUCTOR_COMPILE_THREADS"] = str(train_env.get("TORCHINDUCTOR_COMPILE_THREADS", "1"))
    
    if args.continue_training:
        train_cmd.append("--c")
        print("ğŸ“‚ å°‡å¾æœ€æ–°çš„ checkpoint ç¹¼çºŒè¨“ç·´...")
    
    subprocess.run(train_cmd, env=train_env, check=True)

    print("\n" + "="*60)
    print("âœ“ è¨“ç·´å®Œæˆï¼")
    print(f"  æ¨¡å‹å·²å„²å­˜æ–¼: {os.environ['NNUNET_RESULTS']}")
    print("="*60)