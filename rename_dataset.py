# cite: Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring 
# method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.

import os
import subprocess
import multiprocessing
import json
import nibabel as nib
import numpy as np
import json as pyjson
import zipfile
import shutil
import tempfile
from urllib.parse import urlparse
import subprocess
import sys
import argparse
try:
    import requests
except Exception:
    requests = None
try:
    import gdown  # for Google Drive links
except Exception:
    gdown = None
try:
    from tqdm import tqdm
except ImportError:
    print("å®‰è£ tqdm ä»¥é¡¯ç¤ºé€²åº¦æ¢...")
    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tqdm'])
    from tqdm import tqdm

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# è·¨å¹³å°ï¼šä½¿ç”¨ç•¶å‰è…³æœ¬æ‰€åœ¨ç›®éŒ„ä½œç‚º base_dir
base_dir = os.path.dirname(os.path.abspath(__file__))
print(f"è…³æœ¬ base_dir: {base_dir}")  # è¨ºæ–·æ‰“å°

# nnU-Net v2 ç’°å¢ƒè®Šæ•¸è¨­å®š
os.environ["NNUNET_RAW"] = os.path.join(base_dir, "nnUNet_raw")
os.environ["NNUNET_PREPROCESSED"] = os.path.join(base_dir, "nnUNet_preprocessed")
os.environ["NNUNET_RESULTS"] = os.path.join(base_dir, "nnUNet_results")

# æ•¸æ“šé›† ID å’Œåç¨±
dataset_id = "001"
dataset_name = "CardiacSeg"
dataset_dir = os.path.join(os.environ["NNUNET_RAW"], f"Dataset{dataset_id}_{dataset_name}")
images_tr_dir = os.path.join(dataset_dir, "imagesTr")
labels_tr_dir = os.path.join(dataset_dir, "labelsTr")
images_ts_dir = os.path.join(dataset_dir, "imagesTs")
os.makedirs(images_tr_dir, exist_ok=True)
os.makedirs(labels_tr_dir, exist_ok=True)
os.makedirs(images_ts_dir, exist_ok=True)

def install_dependencies():
    # ä¿ç•™ç‚ºç©ºï¼šç’°å¢ƒå®‰è£æ”¹ç”± setup_environment.ps1 è™•ç†
    return

def _is_already_standard(fname: str, is_image: bool) -> bool:
    if not fname.endswith('.nii.gz'):
        return False
    if is_image:
        return fname.count('_') >= 2 and fname.endswith('_0000.nii.gz')
    # label: should not contain _gt and not end with _0000
    return (not fname.endswith('_0000.nii.gz')) and ('_gt' not in fname)

def _all_files_standard(images_tr_dir, labels_tr_dir, images_ts_dir) -> bool:
    for f in os.listdir(images_tr_dir):
        if f.endswith('.nii.gz') and not _is_already_standard(f, True):
            return False
    for f in os.listdir(labels_tr_dir):
        if f.endswith('.nii.gz') and not _is_already_standard(f, False):
            return False
    for f in os.listdir(images_ts_dir):
        if f.endswith('.nii.gz') and not _is_already_standard(f, True):
            return False
    return True

def _quick_dataset_json_ok(json_path: str, images_tr_dir: str, labels_tr_dir: str) -> bool:
    if not os.path.exists(json_path):
        return False
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            js = json.load(f)
        if 'labels' not in js or 'background' not in js['labels']:
            return False
        # basic count match
        train_images = [x for x in os.listdir(images_tr_dir) if x.endswith('_0000.nii.gz')]
        train_labels = [x for x in os.listdir(labels_tr_dir) if x.endswith('.nii.gz') and not x.endswith('_0000.nii.gz')]
        if len(js.get('training', [])) != min(len(train_images), len(train_labels)):
            return False
        return True
    except Exception:
        return False

def generate_dataset_json(force_regenerate=False, fast_skip=True):
    json_path = os.path.join(dataset_dir, "dataset.json")
    images_tr_dir = os.path.join(dataset_dir, "imagesTr")
    labels_tr_dir = os.path.join(dataset_dir, "labelsTr")
    images_ts_dir = os.path.join(dataset_dir, "imagesTs")

    # å¿«é€Ÿè·³éï¼šæª”åéƒ½æ¨™æº– & dataset.json æœ‰æ•ˆ ä¸”æœªè¦æ±‚å¼·åˆ¶
    if fast_skip and not force_regenerate and _all_files_standard(images_tr_dir, labels_tr_dir, images_ts_dir) and _quick_dataset_json_ok(json_path, images_tr_dir, labels_tr_dir):
        print("âœ“ æª¢æŸ¥ï¼šæª”æ¡ˆèˆ‡ dataset.json å‡å·²æ¨™æº–ï¼Œå¿«é€Ÿè·³éé‡å»ºã€‚")
        return

    if os.path.exists(json_path) and not force_regenerate:
        print(f"dataset.json å·²å­˜åœ¨æ–¼ {json_path}ï¼Œä½†å› æª¢æŸ¥æœªé”å¿«é€Ÿè·³éæ¢ä»¶ï¼Œé‡æ–°æƒæç”Ÿæˆã€‚")

    training = []
    all_labels = set()

    print("\nè™•ç†è¨“ç·´å½±åƒå’Œæ¨™ç±¤...")
    img_files = sorted([f for f in os.listdir(images_tr_dir) if f.endswith('.nii.gz')])
    for img_file in tqdm(img_files, desc="ğŸ”„ æƒæè¨“ç·´è³‡æ–™", unit="æª”æ¡ˆ"):
        if '_' not in img_file or not img_file.split('_')[-1].startswith('0000'):
            old_img = os.path.join(images_tr_dir, img_file)
            new_img_file = img_file.replace('.nii.gz', '_0000.nii.gz')
            new_img = os.path.join(images_tr_dir, new_img_file)
            os.rename(old_img, new_img)
            tqdm.write(f"  âœ“ é‡å‘½åå½±åƒï¼š{img_file} -> {new_img_file}")
            std_img_file = new_img_file
        else:
            std_img_file = img_file

        base_case = std_img_file.replace('_0000.nii.gz', '').replace('.nii.gz', '')
        label_candidates = [f for f in os.listdir(labels_tr_dir) if f.startswith(base_case) and f.endswith('.nii.gz')]
        if not label_candidates:
            continue
        original_label = label_candidates[0]
        std_label_file = f"{base_case}.nii.gz"
        old_label = os.path.join(labels_tr_dir, original_label)
        new_label = os.path.join(labels_tr_dir, std_label_file)
        if original_label != std_label_file:
            os.rename(old_label, new_label)
            tqdm.write(f"  âœ“ é‡å‘½åæ¨™ç±¤ï¼š{original_label} -> {std_label_file}")

        training.append({"image": f"./imagesTr/{std_img_file}", "label": f"./labelsTr/{std_label_file}"})
        try:
            lbl_img = nib.load(new_label)
            lbl_data = lbl_img.get_fdata()
            unique_labels = np.unique(lbl_data)
            # ä»¥ int() è½‰æˆåŸç”Ÿ Python int
            for ul in unique_labels:
                try:
                    all_labels.add(int(ul))
                except Exception:
                    pass
        except Exception as e:
            tqdm.write(f"  âš  è¼‰å…¥æ¨™ç±¤å¤±æ•— {new_label}: {e}")

    print("\nè™•ç†æ¸¬è©¦å½±åƒ...")
    test = []
    test_files = sorted([f for f in os.listdir(images_ts_dir) if f.endswith('.nii.gz')])
    for f in tqdm(test_files, desc="ğŸ”„ æƒææ¸¬è©¦è³‡æ–™", unit="æª”æ¡ˆ"):
        if '_' not in f or not f.split('_')[-1].startswith('0000'):
            # é‡å‘½åæ¸¬è©¦å½±åƒä¹ŸåŠ  _0000
            old_test = os.path.join(images_ts_dir, f)
            new_test_file = f.replace('.nii.gz', '_0000.nii.gz')
            new_test = os.path.join(images_ts_dir, new_test_file)
            if not os.path.exists(new_test):
                try:
                    os.rename(old_test, new_test)
                    tqdm.write(f"  âœ“ é‡å‘½åæ¸¬è©¦å½±åƒï¼š{f} -> {new_test_file}")
                except FileExistsError:
                    pass
            test.append(f"./imagesTs/{new_test_file}")
        else:
            test.append(f"./imagesTs/{f}")

    # å»ºç«‹æ¨™ç±¤åç¨± (èƒŒæ™¯ + å…¶å®ƒ) æ­£ç¢ºæ–¹å‘: åç¨± -> æ•´æ•¸å€¼
    label_name_map = {"background": 0}
    for l in sorted(all_labels):
        if l == 0:
            continue
        label_name_map[f"label{l}"] = int(l)

    dataset_json = {
        "channel_names": {"0": "CT"},
        "labels": label_name_map,
        "numTraining": len(training),
        "file_ending": ".nii.gz",
        "name": dataset_name,
        "description": "Cardiac segmentation dataset",
        "reference": "",
        "licence": "",
        "release": "1.0",
        "tensorImageSize": "3D",
        "training": training,
        "test": test
    }

    def _normalize(o):
        if isinstance(o, dict):
            return {str(k): _normalize(v) for k, v in o.items()}
        if isinstance(o, list):
            return [_normalize(v) for v in o]
        if isinstance(o, (np.integer,)):
            return int(o)
        if isinstance(o, (np.floating,)):
            return float(o)
        return o

    dataset_json = _normalize(dataset_json)
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(dataset_json, f, indent=4, ensure_ascii=False)
    print(f"\nâœ“ å·²ç”Ÿæˆ/æ›´æ–° dataset.json æ–¼ {json_path}")
    print(f"  - è¨“ç·´è³‡æ–™æ•¸é‡ï¼š{len(training)}")
    print(f"  - æ¸¬è©¦è³‡æ–™æ•¸é‡ï¼š{len(test)}")
    print(f"  - æª¢æ¸¬åˆ°çš„æ¨™ç±¤ï¼š{sorted(all_labels)}")

def _has_data():
    def _has_nii(p):
        return os.path.isdir(p) and any(fn.endswith('.nii') or fn.endswith('.nii.gz') for fn in os.listdir(p))
    return _has_nii(images_tr_dir) and _has_nii(labels_tr_dir)

def _download_file(url, dst):
    # Prefer gdown for Google Drive links
    if 'drive.google.com' in url or 'uc?id=' in url:
        gd = gdown
        if gd is None:
            try:
                subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'gdown==4.6.0'])
                import gdown as gd  # type: ignore
            except Exception as e:
                raise RuntimeError(f"ç„¡æ³•å®‰è£ gdown ç”¨æ–¼ Google Drive ä¸‹è¼‰: {e}")
        try:
            print(f"ğŸ“¥ ä¸‹è¼‰ä¸­ï¼š{os.path.basename(dst)}")
            gd.download(url, dst, quiet=False)
            if not os.path.exists(dst) or os.path.getsize(dst) == 0:
                raise RuntimeError("gdown ä¸‹è¼‰çµæœç„¡æ•ˆæˆ–ç‚ºç©ºæª”æ¡ˆã€‚")
            return
        except Exception as e:
            raise RuntimeError(f"gdown ä¸‹è¼‰å¤±æ•—: {e}")
    # Fallback to requests for regular URLs
    if requests is None:
        raise RuntimeError("requests æœªå®‰è£ï¼Œç„¡æ³•ä¸‹è¼‰ã€‚è«‹å…ˆå®‰è£æˆ–æ”¹ç”¨æœ¬æ©Ÿæª”æ¡ˆä¾†æºã€‚")
    print(f"ğŸ“¥ ä¸‹è¼‰ä¸­ï¼š{os.path.basename(dst)}")
    with requests.get(url, stream=True, timeout=60) as r:
        r.raise_for_status()
        total_size = int(r.headers.get('content-length', 0))
        with open(dst, 'wb') as f:
            with tqdm(total=total_size, unit='B', unit_scale=True, desc=os.path.basename(dst)) as pbar:
                for chunk in r.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))

def _extract_zip(zip_path, target_dir):
    print(f"ğŸ“¦ è§£å£“ç¸®ï¼š{os.path.basename(zip_path)}")
    with zipfile.ZipFile(zip_path, 'r') as zf:
        members = zf.namelist()
        for member in tqdm(members, desc="è§£å£“ç¸®æª”æ¡ˆ", unit="æª”æ¡ˆ"):
            zf.extract(member, target_dir)

def _copy_nii_tree(src_dir, dst_dir):
    os.makedirs(dst_dir, exist_ok=True)
    nii_files = []
    for root, _, files in os.walk(src_dir):
        for fn in files:
            if fn.endswith('.nii') or fn.endswith('.nii.gz'):
                nii_files.append((root, fn))
    
    if nii_files:
        print(f"ğŸ“‹ è¤‡è£½ {len(nii_files)} å€‹ NIfTI æª”æ¡ˆåˆ° {os.path.basename(dst_dir)}")
        for root, fn in tqdm(nii_files, desc="è¤‡è£½æª”æ¡ˆ", unit="æª”æ¡ˆ"):
            src = os.path.join(root, fn)
            dst = os.path.join(dst_dir, fn)
            if not os.path.exists(dst):
                shutil.copy2(src, dst)

def _find_candidate_dirs(root_dir):
    image_dirs = []
    label_dirs = []
    test_dirs = []
    for current_root, dirs, _ in os.walk(root_dir):
        name = os.path.basename(current_root).lower()
        if any(k in name for k in ['imagestr', 'image_tr', 'train_images', 'training_image', 'trainimage', 'image']):
            image_dirs.append(current_root)
        if any(k in name for k in ['labelstr', 'label_tr', 'train_labels', 'training_label', 'gt', 'label']):
            label_dirs.append(current_root)
        if any(k in name for k in ['imagests', 'image_ts', 'test_images', 'testing_image', 'testimage', 'images_ts']):
            test_dirs.append(current_root)
    return image_dirs, label_dirs, test_dirs

def ensure_original_data():
    if _has_data():
        print("åµæ¸¬åˆ°æ—¢æœ‰è³‡æ–™ï¼Œè·³éä¸‹è¼‰ã€‚")
        return
    print("æœªæ‰¾åˆ°åŸå§‹è³‡æ–™ã€‚å˜—è©¦ä¾è¨­å®šä¸‹è¼‰ä¸¦æ•´ç†è‡³ nnU-Net çµæ§‹...")

    # è®€å–å¯é¸çš„è¨­å®šæª” data_sources.jsonï¼ˆä½æ–¼è…³æœ¬ç›®éŒ„ï¼‰
    config_path = os.path.join(base_dir, 'data_sources.json')
    cfg = None
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            cfg = pyjson.load(f)

    # æ”¯æ´å…©ç¨®è¨­å®šï¼š
    # 1) dataset_zip: åŒ…å« imagesTr/labelsTr/imagesTs çµæ§‹çš„å£“ç¸®æª” URL æˆ–æœ¬æ©Ÿè·¯å¾‘
    # 2) train_images_url / train_labels_url / test_images_urlï¼šå„è‡ªçš„å£“ç¸®æª”æˆ–è³‡æ–™å¤¾ URL/è·¯å¾‘
    def _is_url(path_or_url):
        try:
            return urlparse(path_or_url).scheme in ('http', 'https')
        except Exception:
            return False

    def _prepare_from_zip(path_or_url):
        tmpdir = tempfile.mkdtemp(prefix='aicup_ds_')
        try:
            zip_path = os.path.join(tmpdir, 'dataset.zip')
            if _is_url(path_or_url):
                print(f"ä¸‹è¼‰è³‡æ–™å£“ç¸®æª”: {path_or_url}")
                _download_file(path_or_url, zip_path)
            else:
                zip_path = path_or_url
            print("è§£å£“ç¸®...")
            _extract_zip(zip_path, tmpdir)
            # å˜—è©¦åŒ¹é…å­è³‡æ–™å¤¾
            _copy_nii_tree(os.path.join(tmpdir, 'imagesTr'), images_tr_dir)
            _copy_nii_tree(os.path.join(tmpdir, 'labelsTr'), labels_tr_dir)
            if os.path.isdir(os.path.join(tmpdir, 'imagesTs')):
                _copy_nii_tree(os.path.join(tmpdir, 'imagesTs'), images_ts_dir)
            # è‹¥æ¨™æº–è³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œå˜—è©¦åŸºæ–¼åç¨±é—œéµå­—æœå°‹
            if not any(os.scandir(images_tr_dir)) or not any(os.scandir(labels_tr_dir)):
                imgs, lbls, tsts = _find_candidate_dirs(tmpdir)
                if imgs:
                    for d in imgs:
                        _copy_nii_tree(d, images_tr_dir)
                if lbls:
                    for d in lbls:
                        _copy_nii_tree(d, labels_tr_dir)
                if tsts:
                    for d in tsts:
                        _copy_nii_tree(d, images_ts_dir)
        finally:
            shutil.rmtree(tmpdir, ignore_errors=True)

    # å„ªå…ˆ dataset_zip
    used = False
    if cfg and 'dataset_zip' in cfg and cfg['dataset_zip']:
        _prepare_from_zip(cfg['dataset_zip'])
        used = True
    else:
        # åˆ†é …ä¾†æº
        tri = (cfg.get('train_images_url') if cfg else os.environ.get('TRAIN_IMAGES_URL'))
        trl = (cfg.get('train_labels_url') if cfg else os.environ.get('TRAIN_LABELS_URL'))
        tsi = (cfg.get('test_images_url') if cfg else os.environ.get('TEST_IMAGES_URL'))
        for name, url_or_path, target in (
            ('train images', tri, images_tr_dir),
            ('train labels', trl, labels_tr_dir),
            ('test images', tsi, images_ts_dir),
        ):
            if not url_or_path:
                continue
            print(f"æº–å‚™ {name} ä¾†æº: {url_or_path}")
            if os.path.isdir(url_or_path):
                _copy_nii_tree(url_or_path, target)
                used = True
            elif url_or_path.endswith('.zip') or _is_url(url_or_path):
                _prepare_from_zip(url_or_path)
                used = True
            else:
                print(f"è­¦å‘Šï¼šæœªçŸ¥çš„ä¾†æºæ ¼å¼ï¼Œå·²è·³é: {url_or_path}")

    if not _has_data():
        print(f"æœªèƒ½è‡ªå‹•ä¸‹è¼‰/æ•´ç†è³‡æ–™ã€‚è«‹åœ¨ {base_dir} å»ºç«‹ data_sources.jsonï¼Œä¾‹å¦‚:\n" \
              "{\n  \"dataset_zip\": \"https://.../your_dataset.zip\"\n}\n" \
              "æˆ–æä¾› train_images_url / train_labels_url / test_images_urlã€‚äº¦å¯æ‰‹å‹•æ”¾å…¥ nnUNet_raw/Dataset001_CardiacSeg åº•ä¸‹ã€‚")
    else:
        print("åŸå§‹è³‡æ–™å·²åˆ°ä½ã€‚")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Normalize dataset and generate nnU-Net dataset.json')
    parser.add_argument('--force', action='store_true', help='å¼·åˆ¶é‡å»º dataset.json èˆ‡æª”å')
    parser.add_argument('--no-fast-skip', action='store_true', help='åœç”¨å¿«é€Ÿè·³éæª¢æŸ¥')
    args = parser.parse_args()

    multiprocessing.freeze_support()
    install_dependencies()
    ensure_original_data()
    generate_dataset_json(force_regenerate=args.force, fast_skip=not args.no_fast_skip)
    print("\n" + "="*60)
    print("âœ“ å®Œæˆï¼šè³‡æ–™æª¢æŸ¥/é‡å‘½åèˆ‡ dataset.json è™•ç†")
    print("  å¾ŒçºŒé è™•ç†/è¨“ç·´è«‹ä½¿ç”¨ nnunet_train.py")
    print("="*60)